import { Mistral } from '@mistralai/mistralai';
import * as fs from 'fs';
import * as path from 'path';
import * as os from 'os';

export interface TranscriptionSegment {
  speaker: string;
  timestamp: number;
  text: string;
}

export interface TranscriptionResult {
  fullText: string;
  segments: TranscriptionSegment[];
}

/**
 * Process audio transcription using Mistral Voxtral Transcribe 2 API
 * Returns structured transcription with speaker detection and timestamps
 * Supports batch processing for WAV, M4A, MP3 formats
 * @param audioBuffer The audio file buffer to transcribe
 * @param sensitiveWords Optional array of keywords/terms for improved transcription accuracy (e.g., proper nouns, technical terms)
 */
export async function processTranscription(audioBuffer: Buffer, sensitiveWords?: string[]): Promise<TranscriptionResult> {
  try {
    const mistralApiKey = process.env.MISTRAL_API_KEY;

    if (!mistralApiKey) {
      // Fallback for development without API key
      return getFallbackTranscription();
    }

    // Create a temporary file for the audio
    const tempDir = os.tmpdir();
    const tempFile = path.join(tempDir, `audio-${Date.now()}.wav`);
    fs.writeFileSync(tempFile, audioBuffer);

    try {
      // Initialize Mistral client
      const client = new Mistral({
        apiKey: mistralApiKey,
      });

      // Read the audio file
      const audioData = fs.readFileSync(tempFile);
      const audioBase64 = audioData.toString('base64');

      // Call Mistral Voxtral Transcribe 2 API with batch processing
      const response = await client.files.upload({
        file: new File([audioData], `audio-${Date.now()}.wav`, { type: 'audio/wav' }),
      });

      // The file has been uploaded, now we need to call the transcription API
      // Note: Voxtral Transcribe 2 requires the file to be processed
      const transcriptionResult = await callVoxtralTranscribe(
        client,
        response.id || '',
        audioBase64,
        tempFile,
        sensitiveWords
      );

      // Parse the response and create segments
      const segments = parseVoxtralResponse(transcriptionResult);

      return {
        fullText: extractFullText(segments),
        segments,
      };
    } finally {
      // Clean up temporary file
      try {
        fs.unlinkSync(tempFile);
      } catch (e) {
        // Ignore cleanup errors
      }
    }
  } catch (error) {
    console.error('Transcription error:', error);
    // Return fallback on error
    return getFallbackTranscription();
  }
}

/**
 * Call Voxtral Transcribe 2 API using Mistral client
 * @param client Mistral client instance
 * @param fileId Uploaded file ID (may not be used depending on API version)
 * @param audioBase64 Base64 encoded audio data
 * @param tempFile Path to temporary audio file
 * @param sensitiveWords Optional keywords for improved transcription accuracy
 */
async function callVoxtralTranscribe(
  client: Mistral,
  fileId: string,
  audioBase64: string,
  tempFile: string,
  sensitiveWords?: string[]
): Promise<any> {
  try {
    // Use the Mistral API for transcription with Voxtral model
    // The actual implementation depends on the Mistral SDK version and API structure
    // For now, we'll implement a basic approach that works with the SDK

    // Read and prepare the file for direct API call
    const audioData = fs.readFileSync(tempFile);

    // Create a FormData-like object for the multipart request
    const formData = new FormData();
    const audioBlob = new Blob([audioData], { type: 'audio/wav' });
    formData.append('file', audioBlob, 'audio.wav');
    formData.append('model', 'voxtral-transcribe-2');
    
    // Add sensitive words as vocabulary hints if provided
    // This helps the model recognize domain-specific terms, proper nouns, etc.
    if (sensitiveWords && sensitiveWords.length > 0) {
      formData.append('vocabulary', JSON.stringify(sensitiveWords));
    }

    // Make direct API call to Mistral Voxtral Transcribe endpoint
    const apiKey = process.env.MISTRAL_API_KEY || '';
    const response = await fetch('https://api.mistral.ai/v1/audio/transcription', {
      method: 'POST',
      headers: {
        Authorization: `Bearer ${apiKey}`,
      },
      body: formData,
    });

    if (!response.ok) {
      throw new Error(`Mistral API error: ${response.statusText}`);
    }

    const data = await response.json();
    return data;
  } catch (error) {
    console.error('Error calling Voxtral Transcribe:', error);
    throw error;
  }
}

/**
 * Parse Mistral Voxtral Transcribe 2 API response
 * Handles text, timestamps, and speaker information
 */
function parseVoxtralResponse(response: any): TranscriptionSegment[] {
  const segments: TranscriptionSegment[] = [];

  // Handle different response formats from Voxtral Transcribe 2
  if (response.segments && Array.isArray(response.segments)) {
    // If response contains segments with timing and speaker info
    response.segments.forEach((segment: any, index: number) => {
      segments.push({
        speaker: segment.speaker || `Speaker ${index % 2 === 0 ? '1' : '2'}`,
        timestamp: Math.round((segment.start || 0) * 1000), // Convert to milliseconds
        text: segment.text || '',
      });
    });
  } else if (response.text && typeof response.text === 'string') {
    // Fallback: treat entire response as single segment
    segments.push({
      speaker: 'Speaker 1',
      timestamp: 0,
      text: response.text,
    });
  } else if (typeof response === 'string') {
    // If response is directly the transcribed text
    segments.push({
      speaker: 'Speaker 1',
      timestamp: 0,
      text: response,
    });
  }

  return segments;
}

/**
 * Extract full text from segments
 */
function extractFullText(segments: TranscriptionSegment[]): string {
  return segments.map((seg) => seg.text).join(' ');
}

/**
 * Fallback transcription when API is unavailable
 */
function getFallbackTranscription(): TranscriptionResult {
  return {
    fullText: '[Transcription not available - API key not configured]',
    segments: [
      {
        speaker: 'Speaker 1',
        timestamp: 0,
        text: '[Transcription not available - API key not configured]',
      },
    ],
  };
}

/**
 * Advanced transcription with speaker diarization
 * Voxtral Transcribe 2 provides speaker detection capabilities
 * @param audioBuffer The audio file buffer to transcribe
 * @param sensitiveWords Optional keywords for improved transcription accuracy
 */
export async function processTranscriptionWithDiarization(audioBuffer: Buffer, sensitiveWords?: string[]): Promise<TranscriptionResult> {
  // Voxtral Transcribe 2 automatically handles speaker diarization
  return processTranscription(audioBuffer, sensitiveWords);
}

/**
 * Get duration of audio in seconds
 * Requires proper audio processing library (e.g., ffprobe)
 */
export async function getAudioDuration(audioBuffer: Buffer): Promise<number> {
  // This would require a proper audio processing library
  // For now, return 0 as placeholder
  // In production, use a library like `ffprobe` or similar
  return 0;
}
